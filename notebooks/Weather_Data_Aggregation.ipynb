{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Weather_Data_Aggregation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPy/I3XKW0DAYs0eJyBuZCG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"5xV9UHhp7lq6","executionInfo":{"status":"ok","timestamp":1633568146126,"user_tz":240,"elapsed":3,"user":{"displayName":"David Harper","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06345028794086099768"}}},"source":["import pandas as pd\n","from google.colab import drive"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6slItP8qO5zB","executionInfo":{"status":"ok","timestamp":1633568218173,"user_tz":240,"elapsed":70407,"user":{"displayName":"David Harper","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06345028794086099768"}},"outputId":"ffa608d1-3476-4be6-8253-d12ea21f127f"},"source":["drive.mount('/content/drive/')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4laGl03Ob7AG","executionInfo":{"status":"ok","timestamp":1633568224454,"user_tz":240,"elapsed":543,"user":{"displayName":"David Harper","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06345028794086099768"}},"outputId":"31cb784f-8fdd-48ca-ffb1-c58cebb997f1"},"source":["%cd /content/drive/Shareddrives/Data606_Energy/data/weather"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Shareddrives/Data606_Energy/data/weather\n"]}]},{"cell_type":"code","metadata":{"id":"LWSKB51AO9Nd","executionInfo":{"status":"ok","timestamp":1633568249789,"user_tz":240,"elapsed":216,"user":{"displayName":"David Harper","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06345028794086099768"}}},"source":["temp_df = pd.read_csv('temperatures_1981-01-01-2021-10-01.csv')\n","pdsi_df = pd.read_csv('pdsi_1981-01-01-2021-10-01.csv')"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sI9Wnsr0gSSD"},"source":["## Get convert data from 2000 onward into standard deviations above average"]},{"cell_type":"code","metadata":{"id":"tjv_qU17yFgV","executionInfo":{"status":"ok","timestamp":1633568261115,"user_tz":240,"elapsed":119,"user":{"displayName":"David Harper","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06345028794086099768"}}},"source":["def standardize_data(data,date_start):\n","  # first, I need to create an array that holds the average as time goes by during the time range of interest\n","  # I need to do the same thing with the standard deviations\n","  cols = data.columns[1:-3] # excluding 1st column and last 3\n","  cols = pd.to_datetime(cols,format='%Y-%m-%d')\n","  data.columns = ['Untitled']+list(cols)+['y','x','State']\n","  cols_for_average = cols[cols<date_start]\n","  df_for_average = data.copy()[cols_for_average]\n","  # specify which columns are to be standardized\n","  cols_for_analysis = cols[cols>=date_start]\n","  data_for_analysis = data.copy()[cols_for_analysis]\n","  # grab the averages and standard deviations by coordinate and by month\n","  month_summaries = {}\n","  for m in range(1,13):\n","    month_columns = cols_for_average[[col.month == m for col in cols_for_average]]\n","    month_avgs = df_for_average[month_columns].mean(axis=1)\n","    month_stds = df_for_average[month_columns].std(axis=1)\n","    month_summaries[f'Averages {m}'] = month_avgs\n","    month_summaries[f'Standard Deviations {m}'] = month_stds\n","    month_columns_standardize = cols_for_analysis[[col.month == m for col in cols_for_analysis]]\n","    # now standardize the data used for analysis using these averages and standard deviations\n","    for col in month_columns_standardize:\n","      data_for_analysis[col] = (data_for_analysis[col]-month_avgs)/month_stds\n","  data_for_analysis[['y','x','State']] = data[['y','x','State']]\n","  month_summaries['x']=data_for_analysis['x']\n","  month_summaries['y']=data_for_analysis['y']\n","  month_summaries['State']=data_for_analysis['State']\n","  return data_for_analysis, pd.DataFrame(month_summaries)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"-H_Raw6E4gYq","executionInfo":{"status":"ok","timestamp":1633568268002,"user_tz":240,"elapsed":789,"user":{"displayName":"David Harper","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06345028794086099768"}}},"source":["standardized_temperatures, temp_summary = standardize_data(temp_df,'2000-01-01')\n","standardized_droughts, drought_summary = standardize_data(pdsi_df,'2000-01-01')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZubiqfE04wZ6","executionInfo":{"status":"ok","timestamp":1633568269508,"user_tz":240,"elapsed":98,"user":{"displayName":"David Harper","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06345028794086099768"}}},"source":["# first I need to make the column names into datetimes so that I can subset stuff on months\n","date_cols = standardized_temperatures.columns[:-3] # emit the last 3 cause they're coordinates and States\n","date_cols = pd.to_datetime(date_cols,format='%Y-%m-%d')\n","standardized_temperatures.columns = list(date_cols)+['y','x','State']\n","# standardize each of the dataframes\n","date_cols = standardized_droughts.columns[:-3] # emit the last 3 cause they're coordinates and States\n","date_cols = pd.to_datetime(date_cols,format='%Y-%m-%d')\n","standardized_droughts.columns = list(date_cols)+['y','x','State']"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"4aSzxJN1xv26","executionInfo":{"status":"ok","timestamp":1633568271078,"user_tz":240,"elapsed":105,"user":{"displayName":"David Harper","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06345028794086099768"}}},"source":["# a simple metric could be the average value for each state at each month - this should probably be revised after some EDA/research\n","temperature_mean_grouped = standardized_temperatures.groupby('State').mean()\n","drought_mean_grouped = standardized_droughts.groupby('State').mean()"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"vBqRN-ef0tRT","executionInfo":{"status":"ok","timestamp":1633568279926,"user_tz":240,"elapsed":6530,"user":{"displayName":"David Harper","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06345028794086099768"}}},"source":["# save the standardized values not aggregated by state yet in case we want to do further analysis at location level\n","standardized_temperatures.to_csv('standardized_temperatures.csv')\n","standardized_droughts.to_csv('standardized_droughts.csv')\n","# save the averages by state\n","temperature_mean_grouped.to_csv('standardized_temperatures_state.csv')\n","drought_mean_grouped.to_csv('standardized_droughts_state.csv')\n","# save the means and standard deviations by location\n","temp_summary.to_csv('temperature_summary.csv')\n","drought_summary.to_csv('drought_summary.csv')"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"r8b2WGXc0kjH"},"source":[""],"execution_count":null,"outputs":[]}]}